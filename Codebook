---
title: "Final Project Code"
output:
  html_document:
    df_print: paged
---

#### Load Packages
```{r, message = FALSE, error = FALSE, warning = FALSE}
library(wepackage)
library(plyr)
library(dplyr)
library(purrr)
library(zoo)
library(corrplot)
library(ggcorrplot)
library(ggbiplot)
library(BBmisc)
library(MLmetrics)
library(olsrr)
library(glmnet)
library(caret)
library(randomForest)
library(xgboost)
library(mlr)
library(factoextra)
library(pracma)
```

#### performance-based clustering
##### step 1: principal component analysis (PCA)
Before clustering, we wanted to understand how performance indicators are related to each other. Principal Component Analysis can reduce dimensionality of a highly correlated dataset, so we can cluster buildings based on their most important performance metrics. 

PCA results identified two groups of highly correlated performance indicators: 1) occupancy, financial occupancy and discount and 2) net arpm and gross arpm. While they are correlated within the groups, they are not correlated bewteen groups. 
```{r}
#principal component analysis
data.pca <- prcomp(data[, c(9:13)], center = T, scale. = T)
data.pca$rotation[, c(1:2)]    #first two PCA
summary(data.pca)              #first two PCA combined explains 88% of the data variations
ggbiplot(data.pca, scale = 0)  #visualize PCs
```

##### step 2: K-means clustering
This section shows the work of building a performance-based clustering. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. That said, this clustering serves as an indicator of building's historical performance.

Using wss and silhouette methods, the optimal number of clusters is 3. The k-means algorithm of 3 clusters runs for 25 times where it chooses the best clustering result. Based on the average performance metrics by cluster, cluster 1 is the cluster of high performers, while cluster 3 is the cluster of the under performers. Clusters might vary (this time maybe cluster 1 is the high performance cluster, next time may be cluster 5) according to the seed set but the results are the same. The wss and silhouette codes are currently commented out for faster running.
```{r, warning = F}
#scale data
data_scaled <- scale(data[,c(11:13)])

##determine number of clusters
  #method 1: look at total withiness
#fviz_nbclust(data_scaled, kmeans, method = "wss") + geom_vline(xintercept = 3, linetype = 2) 

  #method 2: silhouette
  #fviz_nbclust(data_scaled, kmeans, method = "silhouette") 

#kmeans clustering with 3 clousters and runs 25 times and chooses the best one.
fit <- kmeans(data_scaled, 6, nstart = 25)

#visualize k-means clustering
fviz_cluster(fit, geom = "point", data = data_scaled)

#append cluster assignment
data_labeled <- dplyr::rename(data.frame(data, fit$cluster), perf_label = fit.cluster)
table(data_labeled$perf_label)
data_labeled %>% ggplot(aes(x = occ, y = discount, color = as.factor(perf_label))) + geom_point()

#check cluster results
ggplot(aes(occ, discount, color = factor(perf_label), label = name), data = data_labeled) + geom_point() 

#determine clusters
aggregate(data_labeled[, 9:13],by=list(fit$cluster),FUN=mean) #(from best to worst): 2,1,5,6,4,3

### reorder the cluster label to cluster 1 being the best cluster and cluster 6 being the worst
#### this is to make the performance_cluster meaningful (as we are going to take average)
data_labeled <-  mutate(data_labeled, performance_cluster = ifelse(perf_label == 2, 1, 
                                                                 ifelse(perf_label == 1, 2, 
                                                                     ifelse(perf_label == 5, 3, 
                                                                        ifelse(perf_label == 6, 4,
                                                                               ifelse(perf_label == 4, 5, 6)))))) %>% select(-perf_label)
#check data again (now the performance cluster actually means something)
head(data_labeled)
```


#### Data Cleaning & Processing
This section checks missing values, creates 6 new features (e.g. tour conversion rate, tour booked every sellable capacity, etc), and 
checks data abnormality. 
```{r}
#check for missing values (no missing values)
tidyr::gather(summarize_each(data_labeled, funs(sum(is.na(.))/n())), key = "feature", value = "missing_pct")
data_labeled <- na.omit(data_labeled)  #drop NAs if any

#create new features
data_labeled <- dplyr::mutate(data_labeled, 
                      tour_conversion = tour_completed / ifelse(tour_booked == 0, 1, tour_booked),
                      tour_booked_per_sellable_capacity = tour_booked / ifelse(sellable_capacity == 0, 1, sellable_capacity),
                      tour_completed_per_sellable_capacity = tour_completed / ifelse(sellable_capacity == 0, 1, sellable_capacity),
                      sales_time_per_desk = total_sales_time / ifelse(desks_sold ==0, 1, desks_sold),
                      avg_signup_per_event = total_attendee / ifelse(total_event == 0, 1, total_event),
                      event_participation_rate = total_attendee / ifelse(occupied_capacity == 0, 1, occupied_capacity))
head(data_labeled)

## check data abnormality
summary(data_labeled)   
##     issue 1: event participation shouldn't be a negative number or anything too high (>100)
data_labeled <- data_labeled[data_labeled$event_participation_rate >= 0 | data_labeled$event_participation_rate <= 100,]  
##     issue 2: tour conversion rate is a percentage so it shouldn't go over 1
data_labeled <- data_labeled[data_labeled$tour_conversion < 2, ]
##     issue 3: shouldn't be any is_mature 
data_labeled <- data_labeled[data_labeled$is_mature != -1, ]
##     issue 4: median lead time abnormality
data_labeled <- data_labeled[!(data_labeled$median_lead_time > 100 & data_labeled$occ < 0.8), ]
##     issue 5: ac_issue_count abnormality (data accuracy)
data_labeled <- data_labeled[data_labeled$ac_issue_count <= 1000, ]
dim(data_labeled)  #9431 x 49
```

#### Numerical Variables Check
Before we build the model, we want to take a look at the distribution of all numeric variables for the purposes of initial selection.
```{r}
## check numerical variable distribution
#### category 1: year, month, performance variables
data_labeled %>%
  select(year, month, net_arpm, gross_arpm, occ, financial_occ, discount) %>% 
  tidyr::gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()

##### category 2: market demand variables
data_labeled %>% 
  select(median_lead_time, rush_booking_pct, tour_booked_per_sellable_capacity, tour_booked_per_sellable_capacity, tour_conversion,
         sales_time_per_desk, churn_rate) %>% 
  tidyr::gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()

##### category 3: Zendesk variables
data_labeled %>% 
  select(printing_issue_count, ac_issue_count, billing_issue_count, maintenance_issue_count, printing_issue_solve_speed, ac_issue_solve_speed, billing_issue_solve_speed, maintenance_issue_solve_speed) %>% 
  tidyr::gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()

#### category 4: event variables
#### community event, office hour event, orientation event, other_event might not be good predictors
data_labeled %>% 
  select(happy_hour_event, tgim_event, business_development_event, food_for_thought_event, office_hour_event, community_event, other_event, total_event, total_attendee, orientation_event, wellness_event, total_event, business_development_event, avg_signup_per_event, event_participation_rate) %>% 
  tidyr::gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()
```


#### Feature Engineering
This section details the steps of feature engineering: turning the character-type metrics into factors, taking 3-month's moving average of metrics, and sanity-checking data. 

The idea of taking moving average of all the metrics is to create a window for prediction. This makes sense from both a business and a technical perspective. From the business perspective, the hypothesis is that the future occupancy is highly dependent on how the building and market performed the past 3 months. From the technical perspective, a 3-month window is needed so that the prediction could be performed ahead of time.
```{r}
#-------------feature engineering--------------
#### 1. change character into factor type
data_labeled$name <- as.factor(data_labeled$name)
data_labeled$year <- as.factor(data_labeled$year)
data_labeled$month <- as.factor(data_labeled$month)
data_labeled$city <- as.factor(data_labeled$city)
data_labeled$market <- as.factor(data_labeled$market)
data_labeled$region <- as.factor(data_labeled$region)
data_labeled$is_mature <- as.factor(data_labeled$is_mature)
head(data_labeled)

#### 2. moving average
model_data <- data_labeled %>% 
  dplyr::group_by(name) %>% 
  dplyr::mutate(historical_occupancy = rollapply(occ, 3, mean, align = 'right', fill = NA),
                performance_cluster = rollapply(performance_cluster, 3, mean, align = 'right', fill = NA),
                median_lead_time = rollapply(median_lead_time, 3, mean, align = 'right', fill = NA),
                rush_booking_pct = rollapply(rush_booking_pct, 3, mean, align = 'right', fill = NA),
                tour_conversion = rollapply(tour_conversion, 3, mean, align = 'right', fill = NA),
                tour_booked = rollapply(tour_booked, 3, mean, align = 'right', fill = NA),
                tour_completed = rollapply(tour_completed, 3, mean, align = 'right', fill = NA),
                desks_sold = rollapply(desks_sold, 3, mean, align = 'right', fill = NA),
                total_sales_time = rollapply(total_sales_time, 3, mean, align = 'right', fill = NA),
                tour_booked_per_sellable_capacity = rollapply(tour_booked_per_sellable_capacity, 3, mean, align = 'right', fill = NA),
                tour_completed_per_sellable_capacity = rollapply(tour_completed_per_sellable_capacity, 3, mean, align = 'right', fill = NA),
                sales_time_per_desk = rollapply(sales_time_per_desk, 3, mean, align = 'right', fill = NA),
                churn_rate = rollapply(churn_rate, 3, mean, align = 'right', fill = NA),
                printing_issue_count = rollapply(printing_issue_count, 3, mean, align = 'right', fill = NA),
                ac_issue_count = rollapply(ac_issue_count, 3, mean, align = 'right', fill = NA),
                billing_issue_count = rollapply(billing_issue_count, 3, mean, align = 'right', fill = NA),
                maintenance_issue_count = rollapply(maintenance_issue_count, 3, mean, align = 'right', fill = NA),
                printing_issue_solve_speed = rollapply(printing_issue_solve_speed, 3, mean, align = 'right', fill =NA),
                ac_issue_solve_speed = rollapply(ac_issue_solve_speed, 3, mean, align = 'right', fill = NA),
                billing_issue_solve_speed = rollapply(billing_issue_solve_speed, 3, mean, align = 'right', fill = NA),
                maintenance_issue_solve_speed = rollapply(maintenance_issue_solve_speed, 3, mean, align = 'right', fill = NA),
                other_event = rollapply(other_event, 3, mean, align = 'right', fill = NA),
                business_development_event = rollapply(business_development_event, 3, mean, align = 'right', fill = NA),
                happy_hour_event = rollapply(happy_hour_event, 3, mean, align = 'right', fill = NA),
                tgim_event = rollapply(tgim_event, 3, mean, align = 'right', fill = NA),
                food_for_thought_event = rollapply(food_for_thought_event, 3, mean, align = 'right', fill = NA),
                office_hour_event = rollapply(office_hour_event, 3, mean, align = 'right', fill = NA),
                orientation_event = rollapply(orientation_event, 3, mean, align = 'right', fill = NA),
                wellness_event = rollapply(wellness_event, 3, mean, align = 'right', fill = NA),
                avg_signup_per_event = rollapply(avg_signup_per_event, 3, mean, align = 'right', fill = NA),
                total_event = rollapply(total_event, 3, mean, align = 'right', fill = NA),
                total_attendee = rollapply(total_attendee, 3, mean, align = 'right', fill = NA),
                event_participation_rate = rollapply(event_participation_rate, 3, mean, align = 'right', fill = NA)) %>% 
  select(-c("net_arpm", "gross_arpm", "financial_occ", "sellable_capacity", "total_capacity", "occupied_capacity", "community_event", "orientation_event", "office_hour_event", "other_event", "tour_booked", "tour_completed", "desks_sold", "total_sales_time", "total_event", "total_attendee"))

model_data <- na.omit(model_data)  #drop NA's from taking moving average
dim(model_data)                    #8435, 34

#### 3. sanity-check model_data
head(model_data)
table(model_data$performance_cluster)  #check averaged performance cluster
#write.csv(model_data, "model_final_data.csv")   #only write data when necessary
```

##### correlation analysis
Before we move onto the modeling part, we want to get a quick look at the correlations of occupancy and features as an initial selection. I divided up the variables into sales, tour, event, and zendesk categories and used the correlation matrices to visualize variable correlation.

From the correlation analysis, median_lead_time, rush_booking_pct, desks_sold, tour_booked_per_sellable_capacity, tour_conversion, event_participation_rate, avg_sign_up_per_event, maintenance_issue_count, and billing_issue_solve_speed are correlated to occupancy. The matrices also identified highly-correlated variable pairs, including tour_booked_per_sellable_capacity and tour_completed_per_sellable_capacity.
```{r}
#### important sales variables
new_data <- model_data %>% filter(date > '2018-01-01')
sales_matrix <- as.matrix(model_data[c("occ","discount", "median_lead_time", "rush_booking_pct", "sales_time_per_desk", "churn_rate")])

sales_matrix %>% 
  cor() %>% 
  round(3) %>% 
  ggcorrplot(lab = T, 
             colors = c("#6D9EC1", "white", "#E46726"),
             outline.col = "white",
             ggtheme = ggplot2::theme_gray) + 
             ggtitle('Occupancy vs. Sales Metrics')

#### important tour variables
tour_matrix <- as.matrix(model_data[c("occ", "tour_booked_per_sellable_capacity", "tour_completed_per_sellable_capacity", "tour_conversion")])

tour_matrix %>% 
  cor() %>% 
  round(3) %>% 
  ggcorrplot(lab = T, 
             colors = c("#6D9EC1", "white", "#E46726"),
             outline.col = "white",
             ggtheme = ggplot2::theme_gray) + 
             ggtitle('Occupancy vs. Tour Metrics')

#### important event variables
event_matrix <- as.matrix(new_data[c("occ","tgim_event",  "wellness_event", "avg_signup_per_event", "event_participation_rate")])

event_matrix %>% 
  cor() %>% 
  round(3) %>% 
  ggcorrplot(lab = T, 
             colors = c("#6D9EC1", "white", "#E46726"),
             outline.col = "white",
             ggtheme = ggplot2::theme_gray) + 
             ggtitle('Occupancy vs. Event Metrics')

#### important zendesk variable
zendesk_matrix <- as.matrix(new_data[c("occ", "ac_issue_count","maintenance_issue_count", "printing_issue_solve_speed","billing_issue_solve_speed", "printing_issue_count" )])

zendesk_matrix %>% 
  cor() %>% 
  round(3) %>% 
  ggcorrplot(lab = T, 
             colors = c("#6D9EC1", "white", "#E46726"),
             outline.col = "white",
             ggtheme = ggplot2::theme_gray) + 
             ggtitle('Occupancy vs. Zendesk Metrics')
```




####      ----------------------------------------- modeling section ------------------------------------------
##### 0. training and testing data split 
We use data from Jan 2015 to Apr 2019 (7147 rows) to train models that predict the occupancy for May-July 2019. The metric we are using to evaluate the model is RMSE (root mean squared errors), the standard deviations of errors, and MAE (mean absolute errors), the average absolute errors.
```{r}
test_df <- model_data %>% filter(date >= "2019-05-01")
train_df <- model_data %>% filter(date < "2019-05-01")
test_df <- na.omit(test_df)
train_df <- na.omit(train_df)
dim(test_df)   #1288 obs
dim(train_df)  #7147 obs
```

##### 1. time series moving average (RMSE = 0.126, MAE = 0.081)
Time series moving average model serves as a simple benchmark to evaluate machine learning models. The idea is to use the past 3 month's building occupancy rate as the prediction for next month's occupancy. The drawback of this model are clear: first, it doesn't capture the trends of the past 3 months. For example, if a building historical occupancy rate has been 80%, 85%, 87%. Then moving average will give us 84%, which is quite counterintuitive given the upward trend. In a more specific context, this drawback means that the model can't predict monthly performances during building's ramp-up stage, where the occupancy changes very drastically.

To illustrate the limitation of this model, I visualize the moving average prediction for a WeWork location `12 E 49th St`. The predictions are good only once the building enters maturity stage.
```{r}
ts_data <- tidyr::drop_na(tidyr::spread(filter(select(model_data, name, date, occ), date >= "2019-02-01" & date <= "2019-07-01"), key = date, value = occ))
dim(ts_data)   #324 rows
head(ts_data)

ts_train <- ts_data[1:4]
ts_test <- ts_data[5:7]
ts_pred <- mutate(ts_train, 
                    `2019-05-01` = (`2019-02-01` + `2019-02-01` + `2019-02-01`)/3,
                    `2019-06-01` = (`2019-03-01` + `2019-04-01` + `2019-05-01`)/3,
                    `2019-07-01` = (`2019-04-01` + `2019-05-01` + `2019-06-01`)/3)
actual5 <- as.vector(ts_test[, "2019-05-01", drop = T])
actual6 <- as.vector(ts_test[, "2019-06-01", drop = T])
actual7 <- as.vector(ts_test[, "2019-07-01", drop = T])
actual <- c(actual5, actual6, actual7)

pred5 <- as.vector(ts_pred[, "2019-05-01", drop = T])
pred6 <- as.vector(ts_pred[, "2019-06-01", drop = T])
pred7 <- as.vector(ts_pred[, "2019-07-01", drop = T])
ts_pred <- c(pred5, pred6, pred7)
ts_results <- data.frame(ts_pred, actual)

##### calculate RMSE
ts_rmse <- RMSE(ts_pred, actual)  
ts_rmse      #0.126

##### calculate Mean Absolute Error
ts_mae <- mean(abs(ts_pred-actual))
ts_mae       #0.081

##### visualize model limitations with an example
case_data <- model_data %>% filter(name == '12 E 49th St') %>% select(occ, date)
case_pred <- movavg(as.numeric(case_data$occ), 3, type = 's')
data.frame(case_pred, case_data) %>% 
  ggplot(aes(date)) +
  geom_line(aes(y = occ, color = "actual occupancy")) + 
  geom_line(aes(y = case_pred, color = "moving average predicted occupancy")) + 
  ggtitle("The predictions are good later as the occupancy stabilizes")
```


####  2. linear regression (RMSE = 0.0821)
All together, I trained four linear regression models. The first model is a linear regression model using all features (except for the historical occupancy, city and region). The second model drops the insiginificant and highly correlated features. The third model is a ridge regression and the fourth is a lasso regression - both of them attempted to prevent overfitting. The best linear regression model is trained using LASSO regression with RMSE = 0.091.

    * 1st: linear regression (all features)
    * 2nd: linear regression (17 features)
    * 3rd: ridge regression
    * 4th: lasso regression
Before we move onto models, let's normalize & hot-encode features.
```{r}
#get rid of unused variables
lm_train <- subset(train_df, select = -c(name, city, market, date, historical_occupancy))
lm_test <- subset(test_df, select = -c(name, city, market, date, historical_occupancy))
#scale data
lm_train <- data.frame(lm_train[1:5], scale(lm_train[, 6:29]))  
lm_test <- data.frame(lm_test[1:5], scale(lm_test[, 6:29]))
#double check on NAs
lm_train <- na.omit(lm_train)   
lm_test <- na.omit(lm_test)
```

The best linear model is the LASSO regression with RMSE of **0.0821** and MAE of ****
**The variables used are**: year, month, region*discount, is_mature, median_lead_time,  rush_booking_pct,  sales_time_per_desk,  tour_booked_per_sellable_capacity, churn_rate, tour_completed_per_sellable_capacity,  printing_issue_count, ac_issue_solve_speed,  maintenance_issue_solve_speed, performance_cluster, event_participation_rate,  avg_signup_per_event
```{r}
#### ------------------------------- linear model 1 (RMSE = 0.08, MAE = 0.06, Rsquared = 0.805) --------------------------------------
#let's build a linear model with all of our variables
lm_model1 <- lm(occ ~., data = lm_train)  #building a linear regression with all the features
summary(lm_model1)                         #check model summary, RSquared = 0.805
par(mfrow = c(2,2))
plot(lm_model1)     # the diagnostic plot suggests that the errors are not normally distributed 

## linear model 1 prediction
lm_pred1 <- predict(lm_model1, lm_test)
actual_occupancy <- test_df$occ
lm_rmse1 <- RMSE(lm_pred1, actual_occupancy)
lm_rmse1           #RMSE = 0.08
lm_mae1 <- mean(abs(lm_pred1 -  actual_occupancy))
lm_mae1            #MAE = 0.06
head(data.frame(lm_pred1, actual_occupancy)) # check predicted vs. actual occupancy


#### ---------------------------------- linear model 2 (RMSE = 0.0803, MAE = 0.603, Rsquared = 0.805)-----------------------------------------
###linear model 2 only used 17 features and dropped all insignificant and highly-correlated variables
###once we took care of the overfitting issue, the RMSE actually improved
lm_model2 <- lm(occ ~ year + month + region + discount + is_mature + median_lead_time  + rush_booking_pct + churn_rate + 
                  sales_time_per_desk + tour_booked_per_sellable_capacity  + tour_completed_per_sellable_capacity  +  
                  printing_issue_count  + maintenance_issue_solve_speed  + 
                  performance_cluster + event_participation_rate + avg_signup_per_event, 
                data = lm_train)
summary(lm_model2)  #R squared = 0.79

#check diagnostic plots
par(mfrow = c(2,2))
plot(lm_model2)

#check collinearity
vif <- ols_vif_tol(lm_model2)   
vif[vif$VIF > 4, ]              #the only variables with high collinearity are year and market variables

#make predictions
lm_pred2 <- as.vector(predict(lm_model2, lm_test))
lm_rmse2 <- RMSE(lm_pred2, actual_occupancy)
lm_rmse2           ##mean squared errors = 0.0803
lm_mae2 <- mean(abs(lm_pred2 - actual_occupancy))
lm_mae2            ### mean absolute errors = 0.0603
head(data.frame(lm_pred2, actual_occupancy)) # check predicted vs. actual occupancy


#### -------------------- linear model 3: Ridge Regression (RMSE = 0.086, MAE = 0.064) -------------------
#     get predictor variables
x <- model.matrix(occ ~ year + month + region + discount + is_mature + median_lead_time  + rush_booking_pct + churn_rate + 
                  sales_time_per_desk + tour_booked_per_sellable_capacity  + tour_completed_per_sellable_capacity  +  
                  printing_issue_count  + maintenance_issue_solve_speed  + 
                  performance_cluster + event_participation_rate + avg_signup_per_event,
                data = lm_train)[,-1]
#     get outcome variable
y <- lm_train$occ

#     compute penalized ridge regression
glmnet(x, y, alpha = 1, lambda = NULL)  #alpha = 1 indicates ridge regression

#     find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 0)
#     display the best lambda value
cv$lambda.min   #0.01387462

#     fit the final model on the training data
lm_model3 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
print(lm_model3)
#     display regression coefficients
coef(lm_model3)

#     make predictions on the test data
x.test <- model.matrix(occ ~ year + month + region + discount + is_mature + median_lead_time  + rush_booking_pct + churn_rate + 
                  sales_time_per_desk + tour_booked_per_sellable_capacity  + tour_completed_per_sellable_capacity  +  
                  printing_issue_count  + maintenance_issue_solve_speed  + 
                  performance_cluster + event_participation_rate + avg_signup_per_event,
                data = lm_test)[,-1]
ridge_predictions <-  as.vector(predict(lm_model3, x.test))
#     model performance metrics
lm_rmse3 <- RMSE(ridge_predictions, test_df$occ)  #RMSE = 0.086
lm_rmse3
lm_mae3 <- mean(abs(ridge_predictions - test_df$occ))
lm_mae3   #mae = 0.064

#### ----------------linear model 4: LASSO Regression, RMSE = 0.0805, MAE = 0.064)  -------------------------------
# Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
cv$lambda.min

# Fit the final model on the training data
lm_model4 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
print(lm_model4)
# Dsiplay regression coefficients
coef(lm_model4)
# Make predictions on the test data
lasso_predictions <- as.vector(predict(lm_model4, x.test))
# Model performance metrics
lm_rmse4 <- RMSE(lasso_predictions, test_df$occ)  ##rmse = 0.0805
lm_rmse4
lm_mae4 <- mean(abs(lasso_predictions- test_df$occ))
lm_mae4  #0.0604

#### compare results
data.frame(lm_rmse1, lm_rmse2, lm_rmse3, lm_rmse4)  #choose linear model 4
data.frame(lm_mae1, lm_mae2, lm_mae3, lm_mae4)      #chose linear model 2
coef(lm_model4)
```

### 3. Random Forest (best RMSE = 0.0508, MAE = 0.0323)

#### default random forest model (RMSE = 0.053, MAE = 0.036)
Here I built a random forest model with seleted variables and the default parameters. Please see the variable importance plot below.
```{r}
# split data
rf_train <- subset(train_df, select = -c(name, city, market, date, year, month, performance_cluster, tour_completed_per_sellable_capacity, median_lead_time))
rf_test <- subset(test_df, select = -c(name, city, market, date, year, month, performance_cluster, tour_completed_per_sellable_capacity, median_lead_time))

# ------------------------------------------------- default random forest model ---------------------------------------------- 
trControl <- trainControl(method = "cv",
    number = 5,
    search = "grid")

set.seed(12)
rf_default <- randomForest(occ ~., data = rf_train, ntree = 200, mtry = 7, important = T)
print(rf_default)
importance(rf_default)
varImpPlot(rf_default,type=2)           #mean decrease in node impurity
rf_pred <- predict(rf_default, rf_test) #make prediction
data.frame(rf_pred, rf_test$occ)        #check predictions
#rmse = 0.0535
rf_default_rmse <- RMSE(rf_pred, rf_test$occ)  
rf_default_rmse
#mae = 0.0359
rf_default_mae <- mean(abs(rf_pred - rf_test$occ))
rf_default_mae

## variable importance
randomForest::getTree(rf_default, k = 1,labelVar=TRUE)
plot(rf_default, type = "simple")
```


#### random forest parameter tuning
Here we attempted to search for the best mtry. I didn't use grid search to prevent overfitting of the random forest model.
```{r}
#----------------------------------------------------- parameter tuning
 ### search for best mtry (mtry = 17)
a=c()
i= 5
for (i in 15:20) {
  rf_model <- randomForest(occ ~., data = rf_train, ntree = 200, mtry = i, importance = TRUE)
  rf_pred1 <- predict(rf_model, rf_test)
  a[i-14] = RMSE(rf_pred1, rf_test$occ)
}
a
plot(15:20,a)

## final random forest model
final_rf <- randomForest(occ ~., data = rf_train, ntree = 200, mtry = 17, important = T)
print(final_rf)
importance(final_rf)
varImpPlot(final_rf,type=2)           #mean decrease in node impurity
final_rf_pred <- predict(final_rf, rf_test) #make prediction
final_rf_rmse <- RMSE(final_rf_pred, rf_test$occ)  #rmse = 0.0508
final_rf_rmse  
final_rf_mae <- mean(abs(final_rf_pred - rf_test$occ)) #mae = 0.0323
final_rf_mae

#### 30.9% have MAE less than 1%, 51.09% have MAE less than 2%, 78.7% have MAE less than 5%, 94.9% have MAE less than 10%. 
count(abs((final_rf_pred - rf_test$occ)) < 0.1)
count(abs((final_rf_pred - rf_test$occ)) < 0.05)
count(abs((final_rf_pred - rf_test$occ)) < 0.02)
count(abs((final_rf_pred - rf_test$occ)) < 0.01)
```


### 4. XGBoost model (RMSE = 0.0512, MAE = 0.0329)
In this section, we will prepare data and build an initial XGBoost model. The RMSE of the initial model is 0.0516. When looking at the variable importance, we realized that this XGBoost model is dominately shaped by the `historical_occupancy` variable, which is the first variable the tree split on.
```{r}
xgb_data <- subset(model_data, select = -c(name, city, market, year, month, performance_cluster))
xgb_train <- xgb_data %>% filter(date < '2019-05-01') %>% select(-date)
xgb_test <- xgb_data %>% filter(date >= '2019-05-01') %>% select(-date)

##dummify variables
train_labels <- xgb_train$occ
test_labels <- xgb_test$occ
new_tr <- model.matrix(~.+0, data = subset(xgb_train, select = -c(occ))) 
new_ts <- model.matrix(~.+0, data = subset(xgb_test, select = -c(occ)))

##preparing xgb matrices
dtrain <- xgb.DMatrix(data = new_tr, label = train_labels) 
dtest <- xgb.DMatrix(data = new_ts, label = test_labels)

#default parameters
params <- list(booster = "gbtree", objective = "reg:linear", eta=0.3, gamma=0.01, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

## best nround
set.seed(1234)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 200, nfold = 10, showsd = T, stratified = T, print_every_n = T,
                early.stop.round = 10, maximize = F) ### best iteration = 22

## default model prediction
xgb1 <- xgb.train(params = params, 
                  data = dtrain, 
                  nrounds = 22, 
                  watchlist = list(val=dtest,train=dtrain), 
                  print_every_n = 10, 
                  early_stop_round = 10, 
                  maximize = F, 
                  eval_metric = "error")

xgbpred <- predict(xgb1, dtest)

xgb_rmse <- RMSE(xgbpred, test_labels) 
xgb_rmse    #0.0516
xgb_mae <- mean(abs(xgbpred - test_labels))
xgb_mae     #0.327

###--------------------------------------- check and evaluate predictions --------------------------------
#results: 94.02% with 0.9 accuracy, 79.8% with 0.95 accuracy, 50.46% with 0.98 accuracy
count(abs((xgbpred - test_labels)) < 0.1)  
count(abs((xgbpred - test_labels)) < 0.05) 
count(abs((xgbpred - test_labels)) < 0.02) 

##variable importance
### the variable importance is strongly skewed towards historical occupancy, which is the first variable the tree split on
mat <- xgb.importance(feature_names = colnames(new_tr), model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:15])
```


### --------------------------------------------- parameter tuning ------------------------------------------------
We will go on to tune the model parameters.

Final parameters are output below,

     * nrounds = 121
     * max_depth = 4
     * eta = 0.1567508
     * lambda = 0.1327131

```{r}
#create tasks
traintask <- makeRegrTask(data = data.frame(xgb_train), target = "occ")
testtask <- makeRegrTask(data = data.frame(xgb_test), target = "occ")
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = testtask)

#set learner
lrn <- makeLearner("regr.xgboost",predict.type = "response")
lrn$par.vals <- list(
             objective="reg:linear",
             eval_metric="error",
             nrounds=1L,
             eta=0.1
)

xgb_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 100, upper = 200),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .3),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

# set search strategy
control <- makeTuneControlRandom(maxit = 5L)

# prevent overfitting using cross-validation 
resample_desc <- makeResampleDesc("CV", iters = 5L)

# a list of parameters tuned
tuned_params <- tuneParams(
  learner = lrn,
  task = traintask,
  resampling = resample_desc,
  par.set = xgb_params,
  control = control,
  show.info = T
)

###check parameters tuned
tuned_params$x   #see a list of tuned parameters
tuned_params$y   #mse.test.mean = 0.0019
```


### ------------------------- final xgboost model (RMSE = 0.0512, MAE = 0.0329)-----------------------------------------------------
```{r}
##tuned parameters
lrn_tune <- setHyperPars(
  learner = lrn,
  par.vals = tuned_params$x
)

#train model
xgmodel <- mlr::train(learner = lrn_tune,task = traintask)

#final prediction
xgb_finalpred <- predict(xgmodel, testtask)
xgb_final_results <- data.frame(xgb_finalpred$data$truth, xgb_finalpred$data$response)
names(xgb_final_results) <- c("actual_occupancy", "xgb_predicted_occupancy")
xgb_final_results

##evaluate
xgb_rmse_final <- RMSE(final_results$actual_occupancy, final_results$xgb_predicted_occupancy) 
xgb_rmse_final    #0.0512
xgb_mae_final <- mean(abs(final_results$actual_occupancy - final_results$xgb_predicted_occupancy))
xgb_mae_final     #0.0329

### check results
#### 29.5% have MAE less than 1%, 48.5% have MAE less than 2%, 81% have MAE less than 5%, 94.3% have MAE less than 10%. 
count(abs((final_results$xgb_predicted_occupancy - final_results$actual_occupancy)) < 0.1) 
count(abs((final_results$xgb_predicted_occupancy - final_results$actual_occupancy)) < 0.05) 
count(abs((final_results$xgb_predicted_occupancy - final_results$actual_occupancy)) < 0.02) 
count(abs((final_results$xgb_predicted_occupancy - final_results$actual_occupancy)) < 0.01)
```

#### feature importance (exclude historical performance)
As discussed, XGBoost model heavily geared towards `historical_occupancy` feature. We are thus interested in knowing how well the XGBoost model can perform without the historical occupancy feature and its variable importance.
```{r}
xgb_data <- subset(model_data, select = -c(name, city, market, year, month, historical_occupancy, performance_cluster, tour_completed_per_sellable_capacity, median_lead_time))
xgb_train <- xgb_data %>% filter(date < '2019-05-01') %>% select(-date)
xgb_test <- xgb_data %>% filter(date >= '2019-05-01') %>% select(-date)

##dummify variables
train_labels <- xgb_train$occ
test_labels <- xgb_test$occ
new_tr <- model.matrix(~.+0, data = subset(xgb_train, select = -c(occ))) 
new_ts <- model.matrix(~.+0, data = subset(xgb_test, select = -c(occ)))

##preparing xgb matrices
dtrain <- xgb.DMatrix(data = new_tr, label = train_labels) 
dtest <- xgb.DMatrix(data = new_ts, label = test_labels)

#default parameters
params <- list(booster = "gbtree", objective = "reg:linear", eta=0.3, gamma=0.01, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

## best nround
set.seed(1234)
xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 200, nfold = 10, showsd = T, stratified = T, print_every_n = T,
                early.stop.round = 10, maximize = F) ### best iteration = 22

## default model prediction
xgb1 <- xgb.train(params = params, 
                  data = dtrain, 
                  nrounds = 52, 
                  watchlist = list(val=dtest,train=dtrain), 
                  print_every_n = 10, 
                  early_stop_round = 10, 
                  maximize = F, 
                  eval_metric = "error")

xgbpred <- predict(xgb1, dtest)

xgb_rmse <- RMSE(xgbpred, test_labels) 
xgb_rmse    #0.0518
xgb_mae <- mean(abs(xgbpred - test_labels))
xgb_mae     #0.331

###--------------------------------------- check and evaluate predictions --------------------------------
count(abs((xgbpred - test_labels)) < 0.05)  #results: 94.6% with 0.9 accuracy, 80.5% with 0.95 accuracy, 51.5% with 0.98 accuracy

##variable importance
mat <- xgb.importance(feature_names = colnames(new_tr), model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:15])
round(mat$Gain,3)
```


### -------------------------------------------- evaluate model prediction -----------------------------------------------------
This section puts together predictions from all models and intends to look at the results retrospectively (table: `all_results`).

      +  By region, the model predicts buildings in Japan and Europe, Middle East & Africa the best and the predictions for buildings in Pacific & China can still be improved. The hypothesis is China and Pacific are relatively newer markets with higher percentage of ramp-up buildings, which contributes to a lower prediction accuracy.
      
      +  By city, here are the MAE for the top WeWork cities: New York (0.022), London (0.022), San Francisco (0.06), Tokyo (0.023)

```{r}
### let's put together all model predictions
all_results <- data.frame(xgbpred, final_rf_pred, lm_pred2, test_df)

####   random forest predictions vs. actual occupancy (most of the outliers are from Region Pacific)
all_results %>% 
  filter(final_rf_pred <= 1) %>% 
  ggplot(aes(occ, final_rf_pred)) + 
  geom_point(aes(colour = region)) + 
  ylab("Predicted Occupancy") + 
  xlab("Actual Occupancy") + 
  geom_abline(size = 1.5, color = "black")

####   region predictions accuracy (region Pacific & China isn't performing that well)
all_results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(region) %>% 
  dplyr::summarise(regional_mae = median(abs(pred_error) * 100)) %>%  ##I used median not mean to minimize outlier influence
  arrange(abs(regional_mae)) %>% 
  ggplot(aes(reorder(region, regional_mae), regional_mae)) +
  geom_bar(stat = "identity", width = 0.7, color = "navy")  + 
  coord_flip() + 
  xlab("Regions") + 
  ylab("Margin of Error")
  

####   top city prediction accuracy:
# New York 97.7%
all_results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(city) %>% 
  dplyr::summarise(city_avg_error = mean(abs(pred_error))) %>% 
  arrange(abs(city_avg_error)) %>% 
  filter(city == "New York")

# London 98.2%
results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(city) %>% 
  dplyr::summarise(city_avg_error = mean(abs(pred_error))) %>% 
  arrange(abs(city_avg_error)) %>% 
  filter(city == "London")

# San Francisco 97.7%
results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(city) %>% 
  dplyr::summarise(city_avg_error = mean(abs(pred_error))) %>% 
  arrange(abs(city_avg_error)) %>% 
  filter(city == "San Francisco")

# Tokyo 97.6%
results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(city) %>% 
  dplyr::summarise(city_avg_error = mean(abs(pred_error))) %>% 
  arrange(abs(city_avg_error)) %>% 
  filter(city == "Tokyo")

#ranking by cities
results %>% 
  mutate(pred_error = final_rf_pred - occ) %>% 
  group_by(city) %>% 
  dplyr::summarise(city_avg_error = mean(abs(pred_error))) %>% 
  arrange(abs(city_avg_error))

model_data %>% filter(region == "China")
test_df %>% distinct(name)
```

#### Region Pacific Analysis
Here is a list of locations with prediction errors higher than 0.2 and their open date. They invariably are buildings that are opened 

   +    Equatorial Plaza (2019-04)
   +    Asia Centre (2019-02)
   +    Seolleung (2018-09)
   +    22 Cross St (2018-11)
```{r}
all_results %>% filter(region == "Pacific") %>% mutate(error = final_rf_pred - occ) %>% filter(error > 0.2)
```
